# ğŸ™ï¸ Speech Emotion Recognition (SER) using BiGRU

## ğŸ“Œ Overview
This project implements a **Speech Emotion Recognition (SER)** system using a **custom dataset (AVV_dataset)** built by combining samples from **TESS, RAVDESS, SAVEE, EMOV-DB, and CREMA-D**.

The pipeline extracts acoustic and statistical features from speech signals and trains a **Bidirectional GRU (BiGRU)** model to classify emotions into 8 categories:

- ğŸ˜¨ Fear
- ğŸ˜ƒ Happy
- ğŸ˜¢ Sad
- ğŸ˜¡ Anger
- ğŸ¤¢ Disgust
- ğŸ˜ Neutral
- ğŸ˜² Surprise
- ğŸŒ€ Others

---

## âš™ï¸ Workflow
### 1. Dataset Preparation
- `AVV\_dataset.zip` is structured with speech samples from multiple datasets.
- Each file is labeled based on emotion.

### 2. Feature Extraction
Run the feature extraction script to compute:
- ğŸ¶ MFCC (Mel Frequency Cepstral Coefficients)
- ğŸ“Š HT-based Approximate Entropy
- ğŸ”‰ Pitch, Energy, Spectral features

â¡ï¸ Output: `nppdp.csv` containing extracted features + emotion labels.

### 3. Model Training (BiGRU)
- `nppdp.csv` is used to train a BiGRU model.
- BiGRU captures **temporal dependencies** and improves classification accuracy.

### 4. Prediction
- The trained model predicts emotions from unseen audio samples.

---

## ğŸ› ï¸ Tech Stack
- **Python 3.12.10**
- **Libraries**:
Â  - `librosa` â†’ audio feature extraction.
Â  - `numpy, pandas` â†’ data processing.
Â  - `scikit-learn` â†’ preprocessing, evaluation.
Â  - `tensorflow / keras` â†’ BiGRU implementation.
Â  - `matplotlib, seaborn` â†’ visualization.

---

## ğŸ“‚ Dataset
The **AVV_dataset** was custom-built to ensure balanced emotion representation.
It merges:
- ğŸµ **TESS** â€“ Toronto Emotional Speech Set
- ğŸµ **RAVDESS** â€“ Ryerson Audio-Visual Database of Emotional Speech and Song
- ğŸµ **SAVEE** â€“ Surrey Audio-Visual Expressed Emotion
- ğŸµ **EMOV-DB** â€“ German Emotional Database
- ğŸµ **CREMA-D** â€“ Crowd-sourced Emotional Multimodal Actors Dataset

ğŸ‘‰ All samples were **standardized** in sampling rate and format, then re-labeled into the **8 emotion categories** defined in this project.

---

## ğŸ–¥ï¸ How to Run
1. **Clone the repository**
Â   ```bash
Â   git clone https://github.com/sSomaVignesh/Speech-Emotion-Recognition.git
Â   cd speech-emotion-recognition
   
2. **Install dependencies**
Â   ```bash
Â   pip install -r requirements.txt

3. **Prepare dataset**
Â   ```bash
Â   unzip AVV_dataset.zip -d dataset/

4. **Extract features**
Â   ```bash
Â   python feature_extraction.py

5. **Train the model**
Â   ```bash
Â   python train_bigru.py

---

## ğŸ“ˆ Results

âœ… Achieved **86.6% accuracy** on the test set.

ğŸ–¼ï¸ Visualizations include:
- Accuracy vs. Epochs
- Loss vs. Epochs
- Confusion Matrix Heatmap

---

## ğŸš€ Future Work

ğŸ”¬ Explore **transformer-based architectures** (e.g., wav2vec, HuBERT).

ğŸŒ Extend dataset with **multilingual & real-world noisy samples**.

ğŸ“± Deploy as a **real-time mobile/web app**.

ğŸ¤ Apply in **healthcare (stress/depression detection)** and **customer support analysis**.
